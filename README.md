# ğŸ“Š SolanaScrapper Dashboard

Un projet complet pour **scraper les donnÃ©es de prix SOLANA**, les stocker, et les afficher dans un **dashboard web interactif** hÃ©bergÃ© sur AWS EC2. Le scraping se fait automatiquement toutes les 5 minutes via un `cron`.

---

## ğŸš€ FonctionnalitÃ©s

 â± Scraping automatique toutes les 5 minutes
 ğŸ’¾ Stockage des donnÃ©es dans un fichier `data.csv`
 ğŸ“Š Dashboard web avec graphique et rapport latÃ©ral
 â˜ï¸ DÃ©ploiement facile sur une instance AWS EC2
 ğŸ” Mise Ã  jour en temps rÃ©el via Dash + crontab

---

## ğŸ—‚ï¸ Structure du projet

ScrappingProject/
â”œâ”€â”€ Scraping.py # ğŸ”§ Scrape les donnÃ©es SOLANA Ã  partir de fichiers .sh  
â”œâ”€â”€ run.sh # ğŸ•’ ExÃ©cute Scraping.py automatiquement via cron  
â”œâ”€â”€ data.csv # ğŸ’½ Fichier CSV contenant l'historique des donnÃ©es  
â”œâ”€â”€ Report.py # ğŸ“‹ Fournit un dictionnaire de mÃ©triques via get_report()  
â”œâ”€â”€ Dashboard.py # ğŸŒ Dashboard Dash (graphiques + panneau latÃ©ral)  
â”œâ”€â”€ cron.log # ğŸ“„ Log des exÃ©cutions cron  
â”œâ”€â”€ dashboard.log # ğŸ“„ Log du dashboard lancÃ© avec nohup  
â””â”€â”€ README.md # ğŸ“˜ Fichier d'information du projet  


---

## ğŸ§° Description des fichiers

- `Scraping.py`  
  â†’ Lit et exÃ©cute les scripts `.sh` pour obtenir les donnÃ©es SOLANA depuis des APIs, structure ces donnÃ©es, et met Ã  jour le fichier `data.csv`.

- `run.sh`  
  â†’ Petit script shell lancÃ© par `crontab` toutes les 5 minutes. Il appelle `Scraping.py`.

- `Report.py`  
  â†’ Contient la fonction `get_report()` qui retourne un dictionnaire formatÃ© pour affichage dans le dashboard.

- `Dashboard.py`  
  â†’ Application Dash qui lit `data.csv` et `get_report()` pour afficher un graphe interactif et un panneau de rapport actualisÃ© automatiquement.

- `data.csv`  
  â†’ Fichier de stockage des donnÃ©es SOLANA (timestamp, open, high, low, last price, volume).
  â†’ IgnorÃ© par le gitignore par souciÃ© future de taille de fichier.

- `cron.log`  
  â†’ Fichier qui stocke les sorties du cron (scraping automatique).

- `dashboard.log`  
  â†’ Fichier qui stocke les logs de lâ€™application Dash (via `nohup`).

---

## âš™ï¸ Installation

### 1. Cloner le dÃ©pÃ´t

git clone https://github.com/lllllllucien/ScrappingProject.git  
cd ScrappingProject  


â± Automatisation avec cron  
1. Ã‰diter la crontab  

crontab -e  
Et ajouter :  

*/5 * * * * /home/ubuntu/ScrappingProject/run.sh >> /home/ubuntu/ScrappingProject/cron.log 2>&1  

2. Contenu du fichier run.sh  
cd /home/ubuntu/ScrappingProject  
/usr/bin/python3 Scraping.py  

3. Rendre le fichier exÃ©cutable  
chmod +x run.sh  

ğŸŒ Lancer le dashboard    
1. Lancer le serveur Dash en arriÃ¨re-plan    
cd /home/ubuntu/ScrappingProject  
nohup python3 Dashboard.py > dashboard.log 2>&1 &  

2. AccÃ©der au dashboard     
Depuis un navigateur :  
http://16.171.134.172:8050/  

âœ… Assure-toi dâ€™avoir ouvert le port 8050 dans le groupe de sÃ©curitÃ© AWS EC2.  

ğŸ›  Commandes utiles  

ğŸ” VÃ©rifier les logs du scraping :  
tail -f cron.log  

ğŸ” VÃ©rifier les logs du dashboard :  
tail -f dashboard.log  

âŒ Stopper le dashboard :  
pkill -f Dashboard.py  

âœ… Tester manuellement le scraping :  
python3 Scraping.py  

ğŸ“Œ Auteur  
Projet dÃ©veloppÃ© par SÃ©vane Bastian et Lucien Bedel dans le cadre d'un projet scolaire.  

ğŸ“„ Licence  
Ce projet est libre d'utilisation pour des fins personnelles, Ã©ducatives ou non commerciales. Contributions bienvenues !  
